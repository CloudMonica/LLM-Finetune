{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {},
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867939a9-ebb1-43d7-b535-291707612354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {},
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {},
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-04 12:20:52.300873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 12:20:53.059999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  1.76it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1564.81 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/environment/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/environment/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.8305, 'grad_norm': 2.20263409614563, 'learning_rate': 4.99375e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6012, 'grad_norm': 3.177042245864868, 'learning_rate': 4.9875000000000006e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4867, 'grad_norm': 3.007197856903076, 'learning_rate': 4.98125e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1232, 'grad_norm': 3.416506767272949, 'learning_rate': 4.975e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1189, 'grad_norm': 2.7558372020721436, 'learning_rate': 4.96875e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8664, 'grad_norm': 2.962233304977417, 'learning_rate': 4.962500000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8385, 'grad_norm': 2.880342960357666, 'learning_rate': 4.95625e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7436, 'grad_norm': 2.9505319595336914, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6316, 'grad_norm': 3.2615201473236084, 'learning_rate': 4.94375e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7178, 'grad_norm': 3.433208703994751, 'learning_rate': 4.937500000000001e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6676, 'grad_norm': 3.6273159980773926, 'learning_rate': 4.93125e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8455, 'grad_norm': 3.8969168663024902, 'learning_rate': 4.9250000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6123, 'grad_norm': 3.5355496406555176, 'learning_rate': 4.91875e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7322, 'grad_norm': 4.476663112640381, 'learning_rate': 4.9125e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6832, 'grad_norm': 3.7028236389160156, 'learning_rate': 4.90625e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7465, 'grad_norm': 3.9793789386749268, 'learning_rate': 4.9e-05, 'epoch': 0.01}\n",
      "{'loss': 3.575, 'grad_norm': 4.126165390014648, 'learning_rate': 4.8937500000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5729, 'grad_norm': 4.316420555114746, 'learning_rate': 4.8875e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5484, 'grad_norm': 4.8343329429626465, 'learning_rate': 4.88125e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5773, 'grad_norm': 4.585995197296143, 'learning_rate': 4.875e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5527, 'grad_norm': 5.052587032318115, 'learning_rate': 4.8687500000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6447, 'grad_norm': 4.086721897125244, 'learning_rate': 4.8625e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6119, 'grad_norm': 4.794460773468018, 'learning_rate': 4.85625e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5107, 'grad_norm': 4.558082580566406, 'learning_rate': 4.85e-05, 'epoch': 0.01}\n",
      "{'loss': 3.473, 'grad_norm': 5.38918399810791, 'learning_rate': 4.8437500000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6016, 'grad_norm': 5.351853847503662, 'learning_rate': 4.8375000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5453, 'grad_norm': 5.354085445404053, 'learning_rate': 4.83125e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6131, 'grad_norm': 4.545154571533203, 'learning_rate': 4.825e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6273, 'grad_norm': 4.767307758331299, 'learning_rate': 4.81875e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5385, 'grad_norm': 5.852800369262695, 'learning_rate': 4.8125000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4662, 'grad_norm': 5.352952480316162, 'learning_rate': 4.80625e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6041, 'grad_norm': 5.763367176055908, 'learning_rate': 4.8e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4146, 'grad_norm': 5.305798530578613, 'learning_rate': 4.79375e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4885, 'grad_norm': 5.422702789306641, 'learning_rate': 4.7875000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5182, 'grad_norm': 5.595531463623047, 'learning_rate': 4.7812500000000003e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5752, 'grad_norm': 5.207070827484131, 'learning_rate': 4.775e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3604, 'grad_norm': 4.900250434875488, 'learning_rate': 4.76875e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5303, 'grad_norm': 5.202271461486816, 'learning_rate': 4.7625000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5205, 'grad_norm': 5.308075428009033, 'learning_rate': 4.7562500000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.474, 'grad_norm': 5.616507530212402, 'learning_rate': 4.75e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6867, 'grad_norm': 5.523918628692627, 'learning_rate': 4.74375e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4947, 'grad_norm': 5.041152000427246, 'learning_rate': 4.7375e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6186, 'grad_norm': 5.64299201965332, 'learning_rate': 4.7312500000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4162, 'grad_norm': 6.461185455322266, 'learning_rate': 4.7249999999999997e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4098, 'grad_norm': 6.24407434463501, 'learning_rate': 4.71875e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4271, 'grad_norm': 5.689751625061035, 'learning_rate': 4.7125e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5297, 'grad_norm': 5.717981338500977, 'learning_rate': 4.7062500000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4434, 'grad_norm': 6.966482639312744, 'learning_rate': 4.7e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4535, 'grad_norm': 5.800213813781738, 'learning_rate': 4.69375e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5633, 'grad_norm': 5.937109470367432, 'learning_rate': 4.6875e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3215, 'grad_norm': 5.768679141998291, 'learning_rate': 4.6812500000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5457, 'grad_norm': 6.746445655822754, 'learning_rate': 4.6750000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5799, 'grad_norm': 5.96907901763916, 'learning_rate': 4.66875e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4836, 'grad_norm': 5.354167938232422, 'learning_rate': 4.6625e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5191, 'grad_norm': 5.453853130340576, 'learning_rate': 4.65625e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6457, 'grad_norm': 5.816876411437988, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4906, 'grad_norm': 5.849867820739746, 'learning_rate': 4.64375e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3707, 'grad_norm': 5.680325984954834, 'learning_rate': 4.6375e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4236, 'grad_norm': 6.194911003112793, 'learning_rate': 4.63125e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4877, 'grad_norm': 6.401548862457275, 'learning_rate': 4.6250000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4312, 'grad_norm': 6.244716644287109, 'learning_rate': 4.61875e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4537, 'grad_norm': 6.783475399017334, 'learning_rate': 4.6125e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4451, 'grad_norm': 5.941538333892822, 'learning_rate': 4.60625e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4504, 'grad_norm': 6.193113327026367, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5258, 'grad_norm': 5.956754684448242, 'learning_rate': 4.59375e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4797, 'grad_norm': 6.410640716552734, 'learning_rate': 4.5875000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5416, 'grad_norm': 6.219493865966797, 'learning_rate': 4.58125e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3008, 'grad_norm': 7.139423370361328, 'learning_rate': 4.575e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3947, 'grad_norm': 6.609808921813965, 'learning_rate': 4.56875e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3545, 'grad_norm': 6.2087721824646, 'learning_rate': 4.5625e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5002, 'grad_norm': 6.9467267990112305, 'learning_rate': 4.55625e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5256, 'grad_norm': 6.821146011352539, 'learning_rate': 4.55e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2465, 'grad_norm': 6.8678436279296875, 'learning_rate': 4.54375e-05, 'epoch': 0.03}\n",
      "{'loss': 3.566, 'grad_norm': 5.755502700805664, 'learning_rate': 4.5375e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4, 'grad_norm': 6.380993843078613, 'learning_rate': 4.5312500000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4766, 'grad_norm': 6.238226890563965, 'learning_rate': 4.525e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6223, 'grad_norm': 6.34581184387207, 'learning_rate': 4.518750000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4693, 'grad_norm': 6.307027816772461, 'learning_rate': 4.5125e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3229, 'grad_norm': 6.485069274902344, 'learning_rate': 4.5062500000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5482, 'grad_norm': 7.017162322998047, 'learning_rate': 4.5e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2902, 'grad_norm': 6.737242221832275, 'learning_rate': 4.49375e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3631, 'grad_norm': 6.472501754760742, 'learning_rate': 4.4875e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4566, 'grad_norm': 7.197603702545166, 'learning_rate': 4.4812500000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4051, 'grad_norm': 6.344778537750244, 'learning_rate': 4.4750000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5039, 'grad_norm': 6.137912273406982, 'learning_rate': 4.46875e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5336, 'grad_norm': 6.244040012359619, 'learning_rate': 4.4625e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2908, 'grad_norm': 7.19580602645874, 'learning_rate': 4.45625e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4914, 'grad_norm': 6.69406270980835, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.451, 'grad_norm': 7.333934783935547, 'learning_rate': 4.44375e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2646, 'grad_norm': 7.7171125411987305, 'learning_rate': 4.4375e-05, 'epoch': 0.03}\n",
      "{'loss': 3.459, 'grad_norm': 7.690418720245361, 'learning_rate': 4.43125e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4199, 'grad_norm': 7.012597560882568, 'learning_rate': 4.4250000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 3.46, 'grad_norm': 7.482606410980225, 'learning_rate': 4.4187500000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5635, 'grad_norm': 7.240754127502441, 'learning_rate': 4.4125e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3598, 'grad_norm': 6.395545482635498, 'learning_rate': 4.40625e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4324, 'grad_norm': 7.9086012840271, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5297, 'grad_norm': 5.8867058753967285, 'learning_rate': 4.3937500000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 3.325, 'grad_norm': 6.981166362762451, 'learning_rate': 4.3875e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4529, 'grad_norm': 7.340322971343994, 'learning_rate': 4.38125e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3916, 'grad_norm': 7.912106990814209, 'learning_rate': 4.375e-05, 'epoch': 0.03}\n",
      " 12%|████▋                                | 1000/8000 [09:59<1:12:16,  1.61it/s]Saving model checkpoint to ./output/checkpoint-1000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4475, 'grad_norm': 6.89328670501709, 'learning_rate': 4.3687500000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4527, 'grad_norm': 7.576112270355225, 'learning_rate': 4.3625e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6461, 'grad_norm': 8.23705768585205, 'learning_rate': 4.35625e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3934, 'grad_norm': 6.6079936027526855, 'learning_rate': 4.35e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3896, 'grad_norm': 8.533886909484863, 'learning_rate': 4.3437500000000006e-05, 'epoch': 0.04}\n",
      "{'loss': 3.357, 'grad_norm': 7.987491130828857, 'learning_rate': 4.3375000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3893, 'grad_norm': 7.199182987213135, 'learning_rate': 4.33125e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4537, 'grad_norm': 7.306455612182617, 'learning_rate': 4.325e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5209, 'grad_norm': 7.119265556335449, 'learning_rate': 4.3187500000000006e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4645, 'grad_norm': 6.658936023712158, 'learning_rate': 4.3125000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3418, 'grad_norm': 6.8515472412109375, 'learning_rate': 4.30625e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5221, 'grad_norm': 7.812276840209961, 'learning_rate': 4.3e-05, 'epoch': 0.04}\n",
      "{'loss': 3.435, 'grad_norm': 7.162276268005371, 'learning_rate': 4.29375e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3566, 'grad_norm': 8.192142486572266, 'learning_rate': 4.2875000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3172, 'grad_norm': 7.5415825843811035, 'learning_rate': 4.28125e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3607, 'grad_norm': 7.374068260192871, 'learning_rate': 4.275e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4473, 'grad_norm': 6.719268321990967, 'learning_rate': 4.26875e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4705, 'grad_norm': 6.289508819580078, 'learning_rate': 4.2625000000000006e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3604, 'grad_norm': 6.5227766036987305, 'learning_rate': 4.25625e-05, 'epoch': 0.04}\n",
      "{'loss': 3.408, 'grad_norm': 6.401163101196289, 'learning_rate': 4.25e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2451, 'grad_norm': 6.553022861480713, 'learning_rate': 4.24375e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3352, 'grad_norm': 7.412138938903809, 'learning_rate': 4.237500000000001e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3744, 'grad_norm': 7.319403171539307, 'learning_rate': 4.23125e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3727, 'grad_norm': 7.681860446929932, 'learning_rate': 4.2250000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4455, 'grad_norm': 6.67227029800415, 'learning_rate': 4.21875e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2807, 'grad_norm': 7.478494167327881, 'learning_rate': 4.2125e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4561, 'grad_norm': 7.174863815307617, 'learning_rate': 4.2062500000000006e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3291, 'grad_norm': 6.926872253417969, 'learning_rate': 4.2e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3895, 'grad_norm': 6.591488838195801, 'learning_rate': 4.19375e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4857, 'grad_norm': 7.315104007720947, 'learning_rate': 4.1875e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4561, 'grad_norm': 6.961266040802002, 'learning_rate': 4.181250000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4498, 'grad_norm': 6.637147426605225, 'learning_rate': 4.175e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4008, 'grad_norm': 10.724655151367188, 'learning_rate': 4.1687500000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3064, 'grad_norm': 7.335514545440674, 'learning_rate': 4.1625e-05, 'epoch': 0.05}\n",
      "{'loss': 3.348, 'grad_norm': 7.412517070770264, 'learning_rate': 4.156250000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3027, 'grad_norm': 8.120026588439941, 'learning_rate': 4.15e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5131, 'grad_norm': 7.438985347747803, 'learning_rate': 4.1437500000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3783, 'grad_norm': 7.006708145141602, 'learning_rate': 4.1375e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3543, 'grad_norm': 6.804039478302002, 'learning_rate': 4.13125e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4166, 'grad_norm': 6.450994968414307, 'learning_rate': 4.125e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3445, 'grad_norm': 7.28340482711792, 'learning_rate': 4.11875e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2625, 'grad_norm': 7.452051639556885, 'learning_rate': 4.1125000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3752, 'grad_norm': 7.563327312469482, 'learning_rate': 4.10625e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3539, 'grad_norm': 7.100099086761475, 'learning_rate': 4.1e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2635, 'grad_norm': 6.685800552368164, 'learning_rate': 4.09375e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3928, 'grad_norm': 7.4166035652160645, 'learning_rate': 4.0875000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4408, 'grad_norm': 10.174280166625977, 'learning_rate': 4.08125e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3004, 'grad_norm': 6.528172016143799, 'learning_rate': 4.075e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4391, 'grad_norm': 7.215666770935059, 'learning_rate': 4.06875e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4529, 'grad_norm': 6.620604038238525, 'learning_rate': 4.0625000000000005e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3375, 'grad_norm': 6.47968864440918, 'learning_rate': 4.0562500000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3857, 'grad_norm': 8.262093544006348, 'learning_rate': 4.05e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4377, 'grad_norm': 8.218056678771973, 'learning_rate': 4.04375e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3977, 'grad_norm': 6.793713569641113, 'learning_rate': 4.0375e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4949, 'grad_norm': 6.945947170257568, 'learning_rate': 4.0312500000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4023, 'grad_norm': 8.037988662719727, 'learning_rate': 4.025e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4615, 'grad_norm': 7.476279258728027, 'learning_rate': 4.01875e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4291, 'grad_norm': 7.536299228668213, 'learning_rate': 4.0125e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5158, 'grad_norm': 9.150164604187012, 'learning_rate': 4.0062500000000005e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3959, 'grad_norm': 7.0141682624816895, 'learning_rate': 4e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3684, 'grad_norm': 7.881989479064941, 'learning_rate': 3.99375e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3602, 'grad_norm': 8.205162048339844, 'learning_rate': 3.9875e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4652, 'grad_norm': 6.933728218078613, 'learning_rate': 3.9812500000000005e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3143, 'grad_norm': 7.80293607711792, 'learning_rate': 3.9750000000000004e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3695, 'grad_norm': 7.35377836227417, 'learning_rate': 3.96875e-05, 'epoch': 0.06}\n",
      "{'loss': 3.2947, 'grad_norm': 6.7958550453186035, 'learning_rate': 3.9625e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4707, 'grad_norm': 8.106051445007324, 'learning_rate': 3.95625e-05, 'epoch': 0.06}\n",
      "{'loss': 3.366, 'grad_norm': 6.905649185180664, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3711, 'grad_norm': 7.239486217498779, 'learning_rate': 3.9437499999999996e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5064, 'grad_norm': 6.771435260772705, 'learning_rate': 3.9375e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4559, 'grad_norm': 7.070272922515869, 'learning_rate': 3.93125e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4939, 'grad_norm': 7.093369960784912, 'learning_rate': 3.9250000000000005e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3992, 'grad_norm': 7.3788933753967285, 'learning_rate': 3.91875e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3986, 'grad_norm': 7.010103702545166, 'learning_rate': 3.9125e-05, 'epoch': 0.06}\n",
      "{'loss': 3.466, 'grad_norm': 7.441030025482178, 'learning_rate': 3.90625e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4354, 'grad_norm': 7.705770015716553, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3588, 'grad_norm': 8.204878807067871, 'learning_rate': 3.8937500000000005e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3457, 'grad_norm': 7.938657283782959, 'learning_rate': 3.8875e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3918, 'grad_norm': 8.101086616516113, 'learning_rate': 3.88125e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3373, 'grad_norm': 7.874695777893066, 'learning_rate': 3.875e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3775, 'grad_norm': 8.673783302307129, 'learning_rate': 3.8687500000000005e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3346, 'grad_norm': 7.453914165496826, 'learning_rate': 3.8625e-05, 'epoch': 0.06}\n",
      "{'loss': 3.573, 'grad_norm': 7.734840393066406, 'learning_rate': 3.85625e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3523, 'grad_norm': 8.247627258300781, 'learning_rate': 3.85e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4902, 'grad_norm': 8.515278816223145, 'learning_rate': 3.8437500000000006e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3779, 'grad_norm': 7.139657497406006, 'learning_rate': 3.8375e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3178, 'grad_norm': 7.836379051208496, 'learning_rate': 3.83125e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3016, 'grad_norm': 7.111179351806641, 'learning_rate': 3.825e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3992, 'grad_norm': 6.877098560333252, 'learning_rate': 3.818750000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3633, 'grad_norm': 7.563718318939209, 'learning_rate': 3.8125e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3811, 'grad_norm': 7.685320854187012, 'learning_rate': 3.8062500000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4719, 'grad_norm': 6.995203971862793, 'learning_rate': 3.8e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2756, 'grad_norm': 7.356119632720947, 'learning_rate': 3.79375e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5004, 'grad_norm': 7.119626522064209, 'learning_rate': 3.7875e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3592, 'grad_norm': 6.692389965057373, 'learning_rate': 3.78125e-05, 'epoch': 0.07}\n",
      "{'loss': 3.284, 'grad_norm': 8.303564071655273, 'learning_rate': 3.775e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3676, 'grad_norm': 7.18216609954834, 'learning_rate': 3.76875e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2281, 'grad_norm': 7.402583599090576, 'learning_rate': 3.7625e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4076, 'grad_norm': 6.964557647705078, 'learning_rate': 3.75625e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4609, 'grad_norm': 7.746862411499023, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.07}\n",
      " 25%|█████████▊                             | 2000/8000 [19:52<57:45,  1.73it/s]/environment/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:15<00:15,  7.59s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:10, 10.72s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:45<00:00, 12.39s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.717 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 30.613206000000005, 'eval_rouge-2': 6.776673999999999, 'eval_rouge-l': 23.434902, 'eval_bleu-4': 0.03291456003004245, 'eval_runtime': 62.6852, 'eval_samples_per_second': 0.798, 'eval_steps_per_second': 0.064, 'epoch': 0.07}\n",
      " 25%|█████████▊                             | 2000/8000 [20:55<57:45,  1.73it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:46<00:00, 12.39s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3842, 'grad_norm': 8.30087947845459, 'learning_rate': 3.74375e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4906, 'grad_norm': 7.251930236816406, 'learning_rate': 3.737500000000001e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5553, 'grad_norm': 8.341151237487793, 'learning_rate': 3.73125e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4877, 'grad_norm': 7.67891788482666, 'learning_rate': 3.7250000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3668, 'grad_norm': 8.030293464660645, 'learning_rate': 3.71875e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3264, 'grad_norm': 7.313260078430176, 'learning_rate': 3.7125e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4396, 'grad_norm': 7.699779033660889, 'learning_rate': 3.70625e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4062, 'grad_norm': 8.206975936889648, 'learning_rate': 3.7e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4367, 'grad_norm': 7.160885334014893, 'learning_rate': 3.69375e-05, 'epoch': 0.07}\n",
      "{'loss': 3.352, 'grad_norm': 7.616482257843018, 'learning_rate': 3.6875e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2832, 'grad_norm': 7.332754135131836, 'learning_rate': 3.68125e-05, 'epoch': 0.07}\n",
      "{'loss': 3.577, 'grad_norm': 7.584784030914307, 'learning_rate': 3.675e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2516, 'grad_norm': 7.041656017303467, 'learning_rate': 3.6687500000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3555, 'grad_norm': 9.182500839233398, 'learning_rate': 3.6625e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3943, 'grad_norm': 7.056140422821045, 'learning_rate': 3.65625e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5203, 'grad_norm': 8.116674423217773, 'learning_rate': 3.65e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3982, 'grad_norm': 6.921344757080078, 'learning_rate': 3.6437500000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4088, 'grad_norm': 7.214858531951904, 'learning_rate': 3.6375e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3484, 'grad_norm': 7.337783336639404, 'learning_rate': 3.63125e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4273, 'grad_norm': 6.897198677062988, 'learning_rate': 3.625e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4486, 'grad_norm': 6.5524115562438965, 'learning_rate': 3.61875e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4096, 'grad_norm': 7.371710300445557, 'learning_rate': 3.6125000000000004e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4115, 'grad_norm': 7.687388896942139, 'learning_rate': 3.60625e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3637, 'grad_norm': 7.644303798675537, 'learning_rate': 3.6e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2309, 'grad_norm': 8.331538200378418, 'learning_rate': 3.59375e-05, 'epoch': 0.08}\n",
      "{'loss': 3.357, 'grad_norm': 7.588496208190918, 'learning_rate': 3.5875000000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4184, 'grad_norm': 8.079020500183105, 'learning_rate': 3.58125e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4686, 'grad_norm': 7.247529983520508, 'learning_rate': 3.575e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2881, 'grad_norm': 8.60671329498291, 'learning_rate': 3.56875e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3426, 'grad_norm': 7.568725109100342, 'learning_rate': 3.5625000000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3133, 'grad_norm': 8.465048789978027, 'learning_rate': 3.5562500000000004e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3184, 'grad_norm': 8.248797416687012, 'learning_rate': 3.55e-05, 'epoch': 0.08}\n",
      "{'loss': 3.36, 'grad_norm': 8.173332214355469, 'learning_rate': 3.54375e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3621, 'grad_norm': 7.168353080749512, 'learning_rate': 3.5375e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2578, 'grad_norm': 7.988395690917969, 'learning_rate': 3.5312500000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3791, 'grad_norm': 7.794064521789551, 'learning_rate': 3.525e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3418, 'grad_norm': 7.597015857696533, 'learning_rate': 3.51875e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4879, 'grad_norm': 8.066276550292969, 'learning_rate': 3.5125e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2256, 'grad_norm': 8.249412536621094, 'learning_rate': 3.5062500000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 3.452, 'grad_norm': 6.995537757873535, 'learning_rate': 3.5e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4477, 'grad_norm': 7.655959606170654, 'learning_rate': 3.49375e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2787, 'grad_norm': 7.452661514282227, 'learning_rate': 3.4875e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3623, 'grad_norm': 6.978627681732178, 'learning_rate': 3.4812500000000006e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3758, 'grad_norm': 7.514816761016846, 'learning_rate': 3.475e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2725, 'grad_norm': 7.219140529632568, 'learning_rate': 3.46875e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3092, 'grad_norm': 7.171277046203613, 'learning_rate': 3.4625e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2559, 'grad_norm': 8.046395301818848, 'learning_rate': 3.45625e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4342, 'grad_norm': 7.310294151306152, 'learning_rate': 3.45e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4691, 'grad_norm': 7.362586975097656, 'learning_rate': 3.4437500000000004e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3973, 'grad_norm': 8.843971252441406, 'learning_rate': 3.4375e-05, 'epoch': 0.09}\n",
      "{'loss': 3.29, 'grad_norm': 7.6905646324157715, 'learning_rate': 3.43125e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3369, 'grad_norm': 8.671686172485352, 'learning_rate': 3.4250000000000006e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2402, 'grad_norm': 7.38789176940918, 'learning_rate': 3.41875e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3889, 'grad_norm': 7.753369331359863, 'learning_rate': 3.4125e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3807, 'grad_norm': 7.086751461029053, 'learning_rate': 3.40625e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3922, 'grad_norm': 7.80411434173584, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4625, 'grad_norm': 7.455759525299072, 'learning_rate': 3.39375e-05, 'epoch': 0.09}\n",
      "{'loss': 3.482, 'grad_norm': 8.184629440307617, 'learning_rate': 3.3875000000000003e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3734, 'grad_norm': 7.78648567199707, 'learning_rate': 3.38125e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4732, 'grad_norm': 8.164995193481445, 'learning_rate': 3.375000000000001e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3451, 'grad_norm': 7.8033905029296875, 'learning_rate': 3.36875e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4133, 'grad_norm': 7.150670528411865, 'learning_rate': 3.3625000000000004e-05, 'epoch': 0.09}\n",
      "{'loss': 3.5201, 'grad_norm': 7.424440383911133, 'learning_rate': 3.35625e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4473, 'grad_norm': 8.1116361618042, 'learning_rate': 3.35e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3965, 'grad_norm': 7.356935977935791, 'learning_rate': 3.34375e-05, 'epoch': 0.09}\n",
      "{'loss': 3.341, 'grad_norm': 7.363545894622803, 'learning_rate': 3.3375e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4088, 'grad_norm': 8.646044731140137, 'learning_rate': 3.33125e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2527, 'grad_norm': 7.0960893630981445, 'learning_rate': 3.325e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4629, 'grad_norm': 7.894637107849121, 'learning_rate': 3.31875e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4324, 'grad_norm': 8.594894409179688, 'learning_rate': 3.3125e-05, 'epoch': 0.09}\n",
      "{'loss': 3.4174, 'grad_norm': 7.761777400970459, 'learning_rate': 3.3062500000000004e-05, 'epoch': 0.09}\n",
      "{'loss': 3.2428, 'grad_norm': 7.120651721954346, 'learning_rate': 3.3e-05, 'epoch': 0.09}\n",
      "{'loss': 3.3832, 'grad_norm': 7.714288234710693, 'learning_rate': 3.29375e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3768, 'grad_norm': 7.798944473266602, 'learning_rate': 3.2875e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4514, 'grad_norm': 8.534859657287598, 'learning_rate': 3.2812500000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3918, 'grad_norm': 7.741421222686768, 'learning_rate': 3.275e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3379, 'grad_norm': 7.654663562774658, 'learning_rate': 3.26875e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2482, 'grad_norm': 7.956827640533447, 'learning_rate': 3.2625e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2717, 'grad_norm': 7.482621192932129, 'learning_rate': 3.25625e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2264, 'grad_norm': 7.310708522796631, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4348, 'grad_norm': 7.651878833770752, 'learning_rate': 3.24375e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3568, 'grad_norm': 7.198336124420166, 'learning_rate': 3.2375e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3826, 'grad_norm': 7.6061787605285645, 'learning_rate': 3.23125e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4398, 'grad_norm': 8.713998794555664, 'learning_rate': 3.2250000000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3988, 'grad_norm': 8.1658935546875, 'learning_rate': 3.21875e-05, 'epoch': 0.1}\n",
      "{'loss': 3.325, 'grad_norm': 8.086468696594238, 'learning_rate': 3.2125e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3602, 'grad_norm': 8.002655029296875, 'learning_rate': 3.20625e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4984, 'grad_norm': 10.072397232055664, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2889, 'grad_norm': 7.558370113372803, 'learning_rate': 3.1937500000000004e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3203, 'grad_norm': 8.615821838378906, 'learning_rate': 3.1875e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2914, 'grad_norm': 7.10980224609375, 'learning_rate': 3.18125e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2328, 'grad_norm': 7.100695610046387, 'learning_rate': 3.175e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3527, 'grad_norm': 8.52009105682373, 'learning_rate': 3.1687500000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2471, 'grad_norm': 7.9106855392456055, 'learning_rate': 3.1624999999999996e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3473, 'grad_norm': 7.803709030151367, 'learning_rate': 3.15625e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2014, 'grad_norm': 8.665179252624512, 'learning_rate': 3.15e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4355, 'grad_norm': 8.313055038452148, 'learning_rate': 3.1437500000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4244, 'grad_norm': 8.378986358642578, 'learning_rate': 3.1375e-05, 'epoch': 0.1}\n",
      "{'loss': 3.4523, 'grad_norm': 7.669924259185791, 'learning_rate': 3.13125e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3627, 'grad_norm': 7.196417808532715, 'learning_rate': 3.125e-05, 'epoch': 0.1}\n",
      " 38%|██████████████▋                        | 3000/8000 [30:49<46:33,  1.79it/s]Saving model checkpoint to ./output/checkpoint-3000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4676, 'grad_norm': 7.648282051086426, 'learning_rate': 3.1187500000000006e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3438, 'grad_norm': 7.5775957107543945, 'learning_rate': 3.1125000000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3412, 'grad_norm': 7.713102340698242, 'learning_rate': 3.10625e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2242, 'grad_norm': 7.334741115570068, 'learning_rate': 3.1e-05, 'epoch': 0.11}\n",
      "{'loss': 3.392, 'grad_norm': 7.729135036468506, 'learning_rate': 3.09375e-05, 'epoch': 0.11}\n",
      "{'loss': 3.5262, 'grad_norm': 7.106010913848877, 'learning_rate': 3.0875000000000005e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2914, 'grad_norm': 7.597491264343262, 'learning_rate': 3.08125e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2916, 'grad_norm': 8.974241256713867, 'learning_rate': 3.075e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2031, 'grad_norm': 8.535483360290527, 'learning_rate': 3.06875e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3885, 'grad_norm': 7.338919162750244, 'learning_rate': 3.0625000000000006e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3369, 'grad_norm': 8.688977241516113, 'learning_rate': 3.05625e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3172, 'grad_norm': 7.6249237060546875, 'learning_rate': 3.05e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3713, 'grad_norm': 8.386961936950684, 'learning_rate': 3.04375e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2965, 'grad_norm': 7.84242582321167, 'learning_rate': 3.0375000000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 3.449, 'grad_norm': 7.769330978393555, 'learning_rate': 3.0312499999999998e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4137, 'grad_norm': 8.104582786560059, 'learning_rate': 3.025e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4793, 'grad_norm': 7.402684688568115, 'learning_rate': 3.0187500000000002e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2168, 'grad_norm': 7.870471000671387, 'learning_rate': 3.0125000000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3545, 'grad_norm': 7.066308975219727, 'learning_rate': 3.00625e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4035, 'grad_norm': 7.777470111846924, 'learning_rate': 3e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3801, 'grad_norm': 8.424300193786621, 'learning_rate': 2.9937500000000003e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3934, 'grad_norm': 7.323286056518555, 'learning_rate': 2.9875000000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4787, 'grad_norm': 8.330918312072754, 'learning_rate': 2.98125e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2127, 'grad_norm': 8.346355438232422, 'learning_rate': 2.975e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4031, 'grad_norm': 7.681181907653809, 'learning_rate': 2.96875e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3574, 'grad_norm': 9.367295265197754, 'learning_rate': 2.9625000000000002e-05, 'epoch': 0.11}\n",
      "{'loss': 3.4094, 'grad_norm': 7.935542583465576, 'learning_rate': 2.9562500000000004e-05, 'epoch': 0.11}\n",
      "{'loss': 3.284, 'grad_norm': 7.517488479614258, 'learning_rate': 2.95e-05, 'epoch': 0.11}\n",
      "{'loss': 3.2961, 'grad_norm': 9.145650863647461, 'learning_rate': 2.94375e-05, 'epoch': 0.11}\n",
      "{'loss': 3.3883, 'grad_norm': 6.918530464172363, 'learning_rate': 2.9375000000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3617, 'grad_norm': 7.506429195404053, 'learning_rate': 2.9312500000000004e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2234, 'grad_norm': 7.802548408508301, 'learning_rate': 2.925e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3963, 'grad_norm': 8.416167259216309, 'learning_rate': 2.91875e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3686, 'grad_norm': 7.524115085601807, 'learning_rate': 2.9125000000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2775, 'grad_norm': 8.606101989746094, 'learning_rate': 2.9062500000000005e-05, 'epoch': 0.12}\n",
      "{'loss': 3.424, 'grad_norm': 7.4182448387146, 'learning_rate': 2.9e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3344, 'grad_norm': 7.289159297943115, 'learning_rate': 2.8937500000000002e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3666, 'grad_norm': 7.285699844360352, 'learning_rate': 2.8875e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2789, 'grad_norm': 8.758965492248535, 'learning_rate': 2.8812500000000002e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3584, 'grad_norm': 7.307425022125244, 'learning_rate': 2.8749999999999997e-05, 'epoch': 0.12}\n",
      "{'loss': 3.1949, 'grad_norm': 8.300677299499512, 'learning_rate': 2.86875e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2809, 'grad_norm': 8.568426132202148, 'learning_rate': 2.8625e-05, 'epoch': 0.12}\n",
      "{'loss': 3.334, 'grad_norm': 9.256631851196289, 'learning_rate': 2.8562500000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2957, 'grad_norm': 8.29723072052002, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3197, 'grad_norm': 8.444307327270508, 'learning_rate': 2.84375e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3367, 'grad_norm': 8.400986671447754, 'learning_rate': 2.8375000000000002e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2604, 'grad_norm': 8.827139854431152, 'learning_rate': 2.8312500000000004e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2752, 'grad_norm': 9.2893705368042, 'learning_rate': 2.825e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4121, 'grad_norm': 7.749309062957764, 'learning_rate': 2.81875e-05, 'epoch': 0.12}\n",
      "{'loss': 3.4395, 'grad_norm': 7.680626392364502, 'learning_rate': 2.8125000000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3875, 'grad_norm': 8.178370475769043, 'learning_rate': 2.80625e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3125, 'grad_norm': 8.518660545349121, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2736, 'grad_norm': 7.769500732421875, 'learning_rate': 2.79375e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2896, 'grad_norm': 8.053858757019043, 'learning_rate': 2.7875e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3555, 'grad_norm': 8.29732608795166, 'learning_rate': 2.7812500000000002e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2055, 'grad_norm': 7.894552707672119, 'learning_rate': 2.7750000000000004e-05, 'epoch': 0.12}\n",
      "{'loss': 3.3117, 'grad_norm': 7.2555952072143555, 'learning_rate': 2.76875e-05, 'epoch': 0.12}\n",
      "{'loss': 3.425, 'grad_norm': 8.8485746383667, 'learning_rate': 2.7625e-05, 'epoch': 0.12}\n",
      "{'loss': 3.2324, 'grad_norm': 8.549797058105469, 'learning_rate': 2.7562500000000002e-05, 'epoch': 0.13}\n",
      "{'loss': 3.185, 'grad_norm': 8.149230003356934, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.13}\n",
      "{'loss': 3.265, 'grad_norm': 10.014044761657715, 'learning_rate': 2.74375e-05, 'epoch': 0.13}\n",
      "{'loss': 3.525, 'grad_norm': 8.198383331298828, 'learning_rate': 2.7375e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3912, 'grad_norm': 8.32944393157959, 'learning_rate': 2.7312500000000003e-05, 'epoch': 0.13}\n",
      "{'loss': 3.199, 'grad_norm': 7.928603649139404, 'learning_rate': 2.725e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3004, 'grad_norm': 9.06480598449707, 'learning_rate': 2.71875e-05, 'epoch': 0.13}\n",
      "{'loss': 3.1953, 'grad_norm': 8.92631721496582, 'learning_rate': 2.7125000000000002e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2994, 'grad_norm': 7.6658220291137695, 'learning_rate': 2.70625e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3697, 'grad_norm': 8.056059837341309, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3922, 'grad_norm': 7.385910511016846, 'learning_rate': 2.6937499999999997e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3975, 'grad_norm': 8.592657089233398, 'learning_rate': 2.6875e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3656, 'grad_norm': 9.19119644165039, 'learning_rate': 2.68125e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3719, 'grad_norm': 9.061744689941406, 'learning_rate': 2.6750000000000003e-05, 'epoch': 0.13}\n",
      "{'loss': 3.1654, 'grad_norm': 10.004215240478516, 'learning_rate': 2.6687499999999998e-05, 'epoch': 0.13}\n",
      "{'loss': 3.4035, 'grad_norm': 8.338606834411621, 'learning_rate': 2.6625e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2191, 'grad_norm': 7.78287410736084, 'learning_rate': 2.6562500000000002e-05, 'epoch': 0.13}\n",
      "{'loss': 3.1992, 'grad_norm': 7.960010528564453, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2385, 'grad_norm': 9.736063003540039, 'learning_rate': 2.6437500000000002e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2738, 'grad_norm': 7.749598026275635, 'learning_rate': 2.6375e-05, 'epoch': 0.13}\n",
      "{'loss': 3.465, 'grad_norm': 7.400264263153076, 'learning_rate': 2.6312500000000003e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2543, 'grad_norm': 7.693279266357422, 'learning_rate': 2.625e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3395, 'grad_norm': 9.442916870117188, 'learning_rate': 2.6187500000000003e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2211, 'grad_norm': 8.272638320922852, 'learning_rate': 2.6124999999999998e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3824, 'grad_norm': 8.41692066192627, 'learning_rate': 2.60625e-05, 'epoch': 0.13}\n",
      "{'loss': 3.3055, 'grad_norm': 8.386110305786133, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.13}\n",
      "{'loss': 3.2795, 'grad_norm': 8.358586311340332, 'learning_rate': 2.5937500000000004e-05, 'epoch': 0.13}\n",
      "{'loss': 3.324, 'grad_norm': 8.189629554748535, 'learning_rate': 2.5875e-05, 'epoch': 0.13}\n",
      "{'loss': 3.457, 'grad_norm': 7.6840996742248535, 'learning_rate': 2.58125e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4764, 'grad_norm': 7.569860458374023, 'learning_rate': 2.5750000000000002e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4135, 'grad_norm': 7.713608741760254, 'learning_rate': 2.5687500000000004e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4404, 'grad_norm': 8.020051956176758, 'learning_rate': 2.5625e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2682, 'grad_norm': 8.192081451416016, 'learning_rate': 2.55625e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3379, 'grad_norm': 8.76130485534668, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 3.377, 'grad_norm': 7.514520645141602, 'learning_rate': 2.54375e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4957, 'grad_norm': 8.211679458618164, 'learning_rate': 2.5375e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3213, 'grad_norm': 8.234718322753906, 'learning_rate': 2.53125e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2795, 'grad_norm': 8.004524230957031, 'learning_rate': 2.525e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4145, 'grad_norm': 7.276195526123047, 'learning_rate': 2.5187500000000002e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3941, 'grad_norm': 8.290719032287598, 'learning_rate': 2.5124999999999997e-05, 'epoch': 0.14}\n",
      "{'loss': 3.435, 'grad_norm': 8.14610481262207, 'learning_rate': 2.50625e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3758, 'grad_norm': 9.056931495666504, 'learning_rate': 2.5e-05, 'epoch': 0.14}\n",
      " 50%|███████████████████▌                   | 4000/8000 [40:42<37:12,  1.79it/s]/environment/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.33s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.82s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.50714, 'eval_rouge-2': 7.867314, 'eval_rouge-l': 25.630082, 'eval_bleu-4': 0.03629476849714136, 'eval_runtime': 37.0471, 'eval_samples_per_second': 1.35, 'eval_steps_per_second': 0.108, 'epoch': 0.14}\n",
      " 50%|███████████████████▌                   | 4000/8000 [41:20<37:12,  1.79it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:20<00:00,  6.88s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-4000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2484, 'grad_norm': 10.438567161560059, 'learning_rate': 2.4937500000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3434, 'grad_norm': 7.743179798126221, 'learning_rate': 2.4875e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4352, 'grad_norm': 8.074563026428223, 'learning_rate': 2.4812500000000003e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3746, 'grad_norm': 8.874540328979492, 'learning_rate': 2.4750000000000002e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2484, 'grad_norm': 8.123686790466309, 'learning_rate': 2.4687500000000004e-05, 'epoch': 0.14}\n",
      "{'loss': 3.302, 'grad_norm': 8.605737686157227, 'learning_rate': 2.4625000000000002e-05, 'epoch': 0.14}\n",
      "{'loss': 3.1668, 'grad_norm': 7.414010524749756, 'learning_rate': 2.45625e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3937, 'grad_norm': 8.268773078918457, 'learning_rate': 2.45e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4182, 'grad_norm': 8.979083061218262, 'learning_rate': 2.44375e-05, 'epoch': 0.14}\n",
      "{'loss': 3.325, 'grad_norm': 8.596000671386719, 'learning_rate': 2.4375e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3537, 'grad_norm': 9.86570930480957, 'learning_rate': 2.43125e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3139, 'grad_norm': 8.129579544067383, 'learning_rate': 2.425e-05, 'epoch': 0.14}\n",
      "{'loss': 3.4115, 'grad_norm': 7.507018089294434, 'learning_rate': 2.4187500000000002e-05, 'epoch': 0.14}\n",
      "{'loss': 3.2756, 'grad_norm': 7.572606563568115, 'learning_rate': 2.4125e-05, 'epoch': 0.14}\n",
      "{'loss': 3.3268, 'grad_norm': 8.324054718017578, 'learning_rate': 2.4062500000000002e-05, 'epoch': 0.14}\n",
      "{'loss': 3.317, 'grad_norm': 8.336624145507812, 'learning_rate': 2.4e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2387, 'grad_norm': 9.198067665100098, 'learning_rate': 2.3937500000000002e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3248, 'grad_norm': 7.930667877197266, 'learning_rate': 2.3875e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4758, 'grad_norm': 9.429489135742188, 'learning_rate': 2.3812500000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3633, 'grad_norm': 8.180044174194336, 'learning_rate': 2.375e-05, 'epoch': 0.15}\n",
      "{'loss': 3.325, 'grad_norm': 7.543268203735352, 'learning_rate': 2.36875e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3105, 'grad_norm': 7.882917404174805, 'learning_rate': 2.3624999999999998e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2869, 'grad_norm': 8.070423126220703, 'learning_rate': 2.35625e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4656, 'grad_norm': 7.822518825531006, 'learning_rate': 2.35e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3885, 'grad_norm': 8.099014282226562, 'learning_rate': 2.34375e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3439, 'grad_norm': 8.28503131866455, 'learning_rate': 2.3375000000000002e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2365, 'grad_norm': 9.018097877502441, 'learning_rate': 2.33125e-05, 'epoch': 0.15}\n",
      "{'loss': 3.1729, 'grad_norm': 8.179306983947754, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2916, 'grad_norm': 7.650697231292725, 'learning_rate': 2.31875e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3869, 'grad_norm': 8.14095401763916, 'learning_rate': 2.3125000000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3838, 'grad_norm': 7.882517337799072, 'learning_rate': 2.30625e-05, 'epoch': 0.15}\n",
      "{'loss': 3.1771, 'grad_norm': 8.344417572021484, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2164, 'grad_norm': 8.604673385620117, 'learning_rate': 2.2937500000000002e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3459, 'grad_norm': 8.86352252960205, 'learning_rate': 2.2875e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2828, 'grad_norm': 8.162809371948242, 'learning_rate': 2.28125e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2646, 'grad_norm': 9.027617454528809, 'learning_rate': 2.275e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3527, 'grad_norm': 8.602164268493652, 'learning_rate': 2.26875e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3754, 'grad_norm': 8.156842231750488, 'learning_rate': 2.2625e-05, 'epoch': 0.15}\n",
      "{'loss': 3.1893, 'grad_norm': 7.479433059692383, 'learning_rate': 2.25625e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3158, 'grad_norm': 8.091178894042969, 'learning_rate': 2.25e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2742, 'grad_norm': 9.503128051757812, 'learning_rate': 2.24375e-05, 'epoch': 0.15}\n",
      "{'loss': 3.3309, 'grad_norm': 7.853078365325928, 'learning_rate': 2.2375000000000002e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4123, 'grad_norm': 8.82365608215332, 'learning_rate': 2.23125e-05, 'epoch': 0.15}\n",
      "{'loss': 3.1162, 'grad_norm': 8.392478942871094, 'learning_rate': 2.2250000000000002e-05, 'epoch': 0.15}\n",
      "{'loss': 3.2525, 'grad_norm': 8.365033149719238, 'learning_rate': 2.21875e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3379, 'grad_norm': 8.759425163269043, 'learning_rate': 2.2125000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4242, 'grad_norm': 9.355003356933594, 'learning_rate': 2.20625e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3525, 'grad_norm': 7.5143961906433105, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3107, 'grad_norm': 9.055338859558105, 'learning_rate': 2.19375e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3658, 'grad_norm': 8.529852867126465, 'learning_rate': 2.1875e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4406, 'grad_norm': 8.837162017822266, 'learning_rate': 2.18125e-05, 'epoch': 0.16}\n",
      "{'loss': 3.1889, 'grad_norm': 10.1171293258667, 'learning_rate': 2.175e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3576, 'grad_norm': 9.392618179321289, 'learning_rate': 2.1687500000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3428, 'grad_norm': 8.2592191696167, 'learning_rate': 2.1625e-05, 'epoch': 0.16}\n",
      "{'loss': 3.1811, 'grad_norm': 8.790657043457031, 'learning_rate': 2.1562500000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3338, 'grad_norm': 9.900495529174805, 'learning_rate': 2.15e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4006, 'grad_norm': 8.256134986877441, 'learning_rate': 2.1437500000000003e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2385, 'grad_norm': 8.286434173583984, 'learning_rate': 2.1375e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3279, 'grad_norm': 9.538708686828613, 'learning_rate': 2.1312500000000003e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2854, 'grad_norm': 9.65928840637207, 'learning_rate': 2.125e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2287, 'grad_norm': 9.46538257598877, 'learning_rate': 2.1187500000000003e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3973, 'grad_norm': 8.362030982971191, 'learning_rate': 2.1125000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3158, 'grad_norm': 10.239958763122559, 'learning_rate': 2.10625e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3703, 'grad_norm': 8.776552200317383, 'learning_rate': 2.1e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2961, 'grad_norm': 7.964900016784668, 'learning_rate': 2.09375e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3848, 'grad_norm': 8.081683158874512, 'learning_rate': 2.0875e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3766, 'grad_norm': 10.030062675476074, 'learning_rate': 2.08125e-05, 'epoch': 0.16}\n",
      "{'loss': 3.5301, 'grad_norm': 9.174362182617188, 'learning_rate': 2.075e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3484, 'grad_norm': 8.92261791229248, 'learning_rate': 2.06875e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4949, 'grad_norm': 8.704444885253906, 'learning_rate': 2.0625e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4277, 'grad_norm': 9.1227388381958, 'learning_rate': 2.0562500000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 3.2861, 'grad_norm': 8.94856071472168, 'learning_rate': 2.05e-05, 'epoch': 0.16}\n",
      "{'loss': 3.4111, 'grad_norm': 7.349091053009033, 'learning_rate': 2.0437500000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2406, 'grad_norm': 8.220990180969238, 'learning_rate': 2.0375e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3949, 'grad_norm': 9.469170570373535, 'learning_rate': 2.0312500000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 3.308, 'grad_norm': 8.13476848602295, 'learning_rate': 2.025e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3168, 'grad_norm': 8.708230972290039, 'learning_rate': 2.01875e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3258, 'grad_norm': 8.825227737426758, 'learning_rate': 2.0125e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4039, 'grad_norm': 9.421907424926758, 'learning_rate': 2.00625e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3848, 'grad_norm': 8.876715660095215, 'learning_rate': 2e-05, 'epoch': 0.17}\n",
      "{'loss': 3.5566, 'grad_norm': 9.15280532836914, 'learning_rate': 1.99375e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2572, 'grad_norm': 9.955305099487305, 'learning_rate': 1.9875000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1766, 'grad_norm': 7.813821792602539, 'learning_rate': 1.98125e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2316, 'grad_norm': 8.167613983154297, 'learning_rate': 1.9750000000000002e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3254, 'grad_norm': 9.93708610534668, 'learning_rate': 1.96875e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1898, 'grad_norm': 8.479057312011719, 'learning_rate': 1.9625000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2561, 'grad_norm': 8.08830738067627, 'learning_rate': 1.95625e-05, 'epoch': 0.17}\n",
      "{'loss': 3.382, 'grad_norm': 8.19769287109375, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3963, 'grad_norm': 7.520610332489014, 'learning_rate': 1.94375e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2943, 'grad_norm': 8.986089706420898, 'learning_rate': 1.9375e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2297, 'grad_norm': 8.40125560760498, 'learning_rate': 1.93125e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3852, 'grad_norm': 8.873330116271973, 'learning_rate': 1.925e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1781, 'grad_norm': 9.224767684936523, 'learning_rate': 1.91875e-05, 'epoch': 0.17}\n",
      "{'loss': 3.3094, 'grad_norm': 8.343036651611328, 'learning_rate': 1.9125e-05, 'epoch': 0.17}\n",
      "{'loss': 3.299, 'grad_norm': 9.366291999816895, 'learning_rate': 1.90625e-05, 'epoch': 0.17}\n",
      "{'loss': 3.4273, 'grad_norm': 8.308694839477539, 'learning_rate': 1.9e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2904, 'grad_norm': 8.454320907592773, 'learning_rate': 1.89375e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2928, 'grad_norm': 8.900007247924805, 'learning_rate': 1.8875e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2166, 'grad_norm': 9.213150024414062, 'learning_rate': 1.88125e-05, 'epoch': 0.17}\n",
      "{'loss': 3.1648, 'grad_norm': 7.991870880126953, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.17}\n",
      " 62%|████████████████████████▍              | 5000/8000 [51:14<28:20,  1.76it/s]Saving model checkpoint to ./output/checkpoint-5000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.4053, 'grad_norm': 9.086736679077148, 'learning_rate': 1.8687500000000004e-05, 'epoch': 0.17}\n",
      "{'loss': 3.2963, 'grad_norm': 9.239412307739258, 'learning_rate': 1.8625000000000002e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2938, 'grad_norm': 7.570586204528809, 'learning_rate': 1.85625e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2502, 'grad_norm': 9.560127258300781, 'learning_rate': 1.85e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2668, 'grad_norm': 9.878716468811035, 'learning_rate': 1.84375e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3965, 'grad_norm': 9.853662490844727, 'learning_rate': 1.8375e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3834, 'grad_norm': 7.655580043792725, 'learning_rate': 1.83125e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3623, 'grad_norm': 8.60417652130127, 'learning_rate': 1.825e-05, 'epoch': 0.18}\n",
      "{'loss': 3.398, 'grad_norm': 7.590357780456543, 'learning_rate': 1.81875e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3049, 'grad_norm': 8.080050468444824, 'learning_rate': 1.8125e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3096, 'grad_norm': 8.090890884399414, 'learning_rate': 1.8062500000000002e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3307, 'grad_norm': 7.95487117767334, 'learning_rate': 1.8e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3221, 'grad_norm': 9.119131088256836, 'learning_rate': 1.7937500000000002e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1594, 'grad_norm': 9.290984153747559, 'learning_rate': 1.7875e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3213, 'grad_norm': 9.243343353271484, 'learning_rate': 1.7812500000000003e-05, 'epoch': 0.18}\n",
      "{'loss': 3.266, 'grad_norm': 9.082331657409668, 'learning_rate': 1.775e-05, 'epoch': 0.18}\n",
      "{'loss': 3.357, 'grad_norm': 9.515328407287598, 'learning_rate': 1.76875e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2348, 'grad_norm': 8.193035125732422, 'learning_rate': 1.7625e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3547, 'grad_norm': 8.689116477966309, 'learning_rate': 1.75625e-05, 'epoch': 0.18}\n",
      "{'loss': 3.376, 'grad_norm': 8.143595695495605, 'learning_rate': 1.75e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3842, 'grad_norm': 10.210959434509277, 'learning_rate': 1.74375e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3355, 'grad_norm': 8.346409797668457, 'learning_rate': 1.7375e-05, 'epoch': 0.18}\n",
      "{'loss': 3.3146, 'grad_norm': 9.917213439941406, 'learning_rate': 1.73125e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4502, 'grad_norm': 8.132539749145508, 'learning_rate': 1.725e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2371, 'grad_norm': 7.8995442390441895, 'learning_rate': 1.71875e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2086, 'grad_norm': 9.56259536743164, 'learning_rate': 1.7125000000000003e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1752, 'grad_norm': 9.204358100891113, 'learning_rate': 1.70625e-05, 'epoch': 0.18}\n",
      "{'loss': 3.0826, 'grad_norm': 8.227198600769043, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.18}\n",
      "{'loss': 3.2932, 'grad_norm': 8.67943000793457, 'learning_rate': 1.6937500000000002e-05, 'epoch': 0.18}\n",
      "{'loss': 3.1145, 'grad_norm': 8.574145317077637, 'learning_rate': 1.6875000000000004e-05, 'epoch': 0.18}\n",
      "{'loss': 3.284, 'grad_norm': 9.583300590515137, 'learning_rate': 1.6812500000000002e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2859, 'grad_norm': 7.948596477508545, 'learning_rate': 1.675e-05, 'epoch': 0.19}\n",
      "{'loss': 3.1904, 'grad_norm': 8.290369033813477, 'learning_rate': 1.66875e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3436, 'grad_norm': 8.649105072021484, 'learning_rate': 1.6625e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2318, 'grad_norm': 8.750243186950684, 'learning_rate': 1.65625e-05, 'epoch': 0.19}\n",
      "{'loss': 3.249, 'grad_norm': 7.9010910987854, 'learning_rate': 1.65e-05, 'epoch': 0.19}\n",
      "{'loss': 3.1818, 'grad_norm': 8.284934997558594, 'learning_rate': 1.64375e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4498, 'grad_norm': 8.12147045135498, 'learning_rate': 1.6375e-05, 'epoch': 0.19}\n",
      "{'loss': 3.1375, 'grad_norm': 8.739717483520508, 'learning_rate': 1.63125e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3254, 'grad_norm': 9.250020980834961, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4158, 'grad_norm': 8.544257164001465, 'learning_rate': 1.61875e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3092, 'grad_norm': 8.46660041809082, 'learning_rate': 1.6125000000000002e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3133, 'grad_norm': 9.536189079284668, 'learning_rate': 1.60625e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2242, 'grad_norm': 9.869482040405273, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3303, 'grad_norm': 8.43406867980957, 'learning_rate': 1.59375e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4008, 'grad_norm': 8.937580108642578, 'learning_rate': 1.5875e-05, 'epoch': 0.19}\n",
      "{'loss': 3.1506, 'grad_norm': 8.932612419128418, 'learning_rate': 1.5812499999999998e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2883, 'grad_norm': 8.844152450561523, 'learning_rate': 1.575e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3213, 'grad_norm': 8.629901885986328, 'learning_rate': 1.56875e-05, 'epoch': 0.19}\n",
      "{'loss': 3.259, 'grad_norm': 8.826333045959473, 'learning_rate': 1.5625e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3148, 'grad_norm': 7.5861616134643555, 'learning_rate': 1.5562500000000002e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3791, 'grad_norm': 8.091888427734375, 'learning_rate': 1.55e-05, 'epoch': 0.19}\n",
      "{'loss': 3.4438, 'grad_norm': 10.579483985900879, 'learning_rate': 1.5437500000000003e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3707, 'grad_norm': 9.649584770202637, 'learning_rate': 1.5375e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2439, 'grad_norm': 8.693912506103516, 'learning_rate': 1.5312500000000003e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2654, 'grad_norm': 9.52125358581543, 'learning_rate': 1.525e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2365, 'grad_norm': 7.805410385131836, 'learning_rate': 1.5187500000000002e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2957, 'grad_norm': 8.717041015625, 'learning_rate': 1.5125e-05, 'epoch': 0.19}\n",
      "{'loss': 3.3252, 'grad_norm': 8.095613479614258, 'learning_rate': 1.5062500000000002e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2855, 'grad_norm': 8.568648338317871, 'learning_rate': 1.5e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3377, 'grad_norm': 8.450006484985352, 'learning_rate': 1.4937500000000002e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2609, 'grad_norm': 8.978588104248047, 'learning_rate': 1.4875e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3617, 'grad_norm': 9.770659446716309, 'learning_rate': 1.4812500000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2969, 'grad_norm': 8.801630973815918, 'learning_rate': 1.475e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3271, 'grad_norm': 9.246665954589844, 'learning_rate': 1.4687500000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4389, 'grad_norm': 8.054139137268066, 'learning_rate': 1.4625e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2891, 'grad_norm': 8.323345184326172, 'learning_rate': 1.4562500000000002e-05, 'epoch': 0.2}\n",
      "{'loss': 3.382, 'grad_norm': 8.633932113647461, 'learning_rate': 1.45e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2895, 'grad_norm': 8.145761489868164, 'learning_rate': 1.44375e-05, 'epoch': 0.2}\n",
      "{'loss': 3.316, 'grad_norm': 9.900175094604492, 'learning_rate': 1.4374999999999999e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4115, 'grad_norm': 10.054230690002441, 'learning_rate': 1.43125e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2557, 'grad_norm': 8.994614601135254, 'learning_rate': 1.4249999999999999e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2855, 'grad_norm': 9.919851303100586, 'learning_rate': 1.4187500000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 3.4064, 'grad_norm': 9.093836784362793, 'learning_rate': 1.4125e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3738, 'grad_norm': 9.091154098510742, 'learning_rate': 1.4062500000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3525, 'grad_norm': 8.93545913696289, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2305, 'grad_norm': 8.70822525024414, 'learning_rate': 1.39375e-05, 'epoch': 0.2}\n",
      "{'loss': 3.31, 'grad_norm': 9.98041820526123, 'learning_rate': 1.3875000000000002e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3898, 'grad_norm': 8.785140037536621, 'learning_rate': 1.38125e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3785, 'grad_norm': 8.59320068359375, 'learning_rate': 1.3750000000000002e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2803, 'grad_norm': 8.459586143493652, 'learning_rate': 1.36875e-05, 'epoch': 0.2}\n",
      "{'loss': 3.1861, 'grad_norm': 8.443194389343262, 'learning_rate': 1.3625e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2211, 'grad_norm': 9.007912635803223, 'learning_rate': 1.3562500000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2527, 'grad_norm': 9.371833801269531, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2932, 'grad_norm': 8.679939270019531, 'learning_rate': 1.34375e-05, 'epoch': 0.2}\n",
      "{'loss': 3.1471, 'grad_norm': 7.763035774230957, 'learning_rate': 1.3375000000000002e-05, 'epoch': 0.2}\n",
      "{'loss': 3.1414, 'grad_norm': 9.216609954833984, 'learning_rate': 1.33125e-05, 'epoch': 0.2}\n",
      "{'loss': 3.3391, 'grad_norm': 7.5340166091918945, 'learning_rate': 1.3250000000000002e-05, 'epoch': 0.21}\n",
      "{'loss': 3.1836, 'grad_norm': 8.693870544433594, 'learning_rate': 1.31875e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2576, 'grad_norm': 8.743578910827637, 'learning_rate': 1.3125e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3934, 'grad_norm': 8.626773834228516, 'learning_rate': 1.3062499999999999e-05, 'epoch': 0.21}\n",
      "{'loss': 3.5232, 'grad_norm': 9.014927864074707, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 3.29, 'grad_norm': 9.892946243286133, 'learning_rate': 1.29375e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4262, 'grad_norm': 8.99791145324707, 'learning_rate': 1.2875000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3879, 'grad_norm': 8.487545013427734, 'learning_rate': 1.28125e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4486, 'grad_norm': 9.839088439941406, 'learning_rate': 1.2750000000000002e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3355, 'grad_norm': 9.647621154785156, 'learning_rate': 1.26875e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3643, 'grad_norm': 7.7762932777404785, 'learning_rate': 1.2625e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3291, 'grad_norm': 8.812966346740723, 'learning_rate': 1.2562499999999999e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4064, 'grad_norm': 8.854187965393066, 'learning_rate': 1.25e-05, 'epoch': 0.21}\n",
      " 75%|███████████████████████████▊         | 6000/8000 [1:01:14<20:07,  1.66it/s]/environment/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.66s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:18<00:07,  7.33s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.317718, 'eval_rouge-2': 6.744298, 'eval_rouge-l': 24.320816, 'eval_bleu-4': 0.032043697770723925, 'eval_runtime': 36.684, 'eval_samples_per_second': 1.363, 'eval_steps_per_second': 0.109, 'epoch': 0.21}\n",
      " 75%|███████████████████████████▊         | 6000/8000 [1:01:51<20:07,  1.66it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:20<00:00,  5.35s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-6000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.2807, 'grad_norm': 8.604695320129395, 'learning_rate': 1.24375e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2369, 'grad_norm': 8.890634536743164, 'learning_rate': 1.2375000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 3.1557, 'grad_norm': 8.746670722961426, 'learning_rate': 1.2312500000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2033, 'grad_norm': 10.120116233825684, 'learning_rate': 1.225e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3906, 'grad_norm': 8.862174034118652, 'learning_rate': 1.21875e-05, 'epoch': 0.21}\n",
      "{'loss': 3.226, 'grad_norm': 10.115422248840332, 'learning_rate': 1.2125e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3463, 'grad_norm': 8.616965293884277, 'learning_rate': 1.20625e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2979, 'grad_norm': 9.950944900512695, 'learning_rate': 1.2e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2896, 'grad_norm': 8.47293472290039, 'learning_rate': 1.19375e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3875, 'grad_norm': 9.665641784667969, 'learning_rate': 1.1875e-05, 'epoch': 0.21}\n",
      "{'loss': 3.2158, 'grad_norm': 9.428831100463867, 'learning_rate': 1.1812499999999999e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3121, 'grad_norm': 11.009612083435059, 'learning_rate': 1.175e-05, 'epoch': 0.21}\n",
      "{'loss': 3.1865, 'grad_norm': 9.19990348815918, 'learning_rate': 1.1687500000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 3.342, 'grad_norm': 7.902392864227295, 'learning_rate': 1.1625000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 3.4291, 'grad_norm': 9.379709243774414, 'learning_rate': 1.1562500000000002e-05, 'epoch': 0.21}\n",
      "{'loss': 3.3064, 'grad_norm': 8.329621315002441, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2912, 'grad_norm': 8.453571319580078, 'learning_rate': 1.14375e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3559, 'grad_norm': 8.620872497558594, 'learning_rate': 1.1375e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2045, 'grad_norm': 8.30933952331543, 'learning_rate': 1.13125e-05, 'epoch': 0.22}\n",
      "{'loss': 3.266, 'grad_norm': 9.21926212310791, 'learning_rate': 1.125e-05, 'epoch': 0.22}\n",
      "{'loss': 3.1693, 'grad_norm': 8.938892364501953, 'learning_rate': 1.1187500000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3697, 'grad_norm': 10.177863121032715, 'learning_rate': 1.1125000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3197, 'grad_norm': 11.077041625976562, 'learning_rate': 1.1062500000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3643, 'grad_norm': 9.01693344116211, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2445, 'grad_norm': 9.797611236572266, 'learning_rate': 1.09375e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3908, 'grad_norm': 9.081831932067871, 'learning_rate': 1.0875e-05, 'epoch': 0.22}\n",
      "{'loss': 3.249, 'grad_norm': 7.935624122619629, 'learning_rate': 1.08125e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2516, 'grad_norm': 9.080181121826172, 'learning_rate': 1.075e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3549, 'grad_norm': 8.928537368774414, 'learning_rate': 1.06875e-05, 'epoch': 0.22}\n",
      "{'loss': 3.2457, 'grad_norm': 9.207521438598633, 'learning_rate': 1.0625e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3912, 'grad_norm': 9.396260261535645, 'learning_rate': 1.0562500000000001e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3379, 'grad_norm': 8.90985107421875, 'learning_rate': 1.05e-05, 'epoch': 0.22}\n",
      "{'loss': 3.315, 'grad_norm': 9.781856536865234, 'learning_rate': 1.04375e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3594, 'grad_norm': 9.879898071289062, 'learning_rate': 1.0375e-05, 'epoch': 0.22}\n",
      "{'loss': 3.326, 'grad_norm': 10.943485260009766, 'learning_rate': 1.03125e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4666, 'grad_norm': 9.932784080505371, 'learning_rate': 1.025e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3547, 'grad_norm': 8.36707878112793, 'learning_rate': 1.01875e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4742, 'grad_norm': 8.408001899719238, 'learning_rate': 1.0125e-05, 'epoch': 0.22}\n",
      "{'loss': 3.359, 'grad_norm': 9.859238624572754, 'learning_rate': 1.00625e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3037, 'grad_norm': 8.429286003112793, 'learning_rate': 1e-05, 'epoch': 0.22}\n",
      "{'loss': 3.3064, 'grad_norm': 8.836248397827148, 'learning_rate': 9.937500000000001e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3283, 'grad_norm': 9.929255485534668, 'learning_rate': 9.875000000000001e-06, 'epoch': 0.22}\n",
      "{'loss': 3.323, 'grad_norm': 8.87088680267334, 'learning_rate': 9.812500000000001e-06, 'epoch': 0.22}\n",
      "{'loss': 3.4203, 'grad_norm': 8.334739685058594, 'learning_rate': 9.750000000000002e-06, 'epoch': 0.22}\n",
      "{'loss': 3.3084, 'grad_norm': 8.321845054626465, 'learning_rate': 9.6875e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2523, 'grad_norm': 8.898348808288574, 'learning_rate': 9.625e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3951, 'grad_norm': 8.818995475769043, 'learning_rate': 9.5625e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3752, 'grad_norm': 8.61594295501709, 'learning_rate': 9.5e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3365, 'grad_norm': 9.36815357208252, 'learning_rate': 9.4375e-06, 'epoch': 0.23}\n",
      "{'loss': 3.275, 'grad_norm': 8.942347526550293, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2512, 'grad_norm': 9.30562686920166, 'learning_rate': 9.312500000000001e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3535, 'grad_norm': 10.589875221252441, 'learning_rate': 9.25e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2584, 'grad_norm': 8.194398880004883, 'learning_rate': 9.1875e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2502, 'grad_norm': 8.393138885498047, 'learning_rate': 9.125e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3094, 'grad_norm': 8.81855297088623, 'learning_rate': 9.0625e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2412, 'grad_norm': 9.480661392211914, 'learning_rate': 9e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3801, 'grad_norm': 8.324991226196289, 'learning_rate': 8.9375e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3664, 'grad_norm': 7.876291275024414, 'learning_rate': 8.875e-06, 'epoch': 0.23}\n",
      "{'loss': 3.4398, 'grad_norm': 8.100194931030273, 'learning_rate': 8.8125e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3459, 'grad_norm': 9.447075843811035, 'learning_rate': 8.75e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2186, 'grad_norm': 10.702394485473633, 'learning_rate': 8.6875e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3072, 'grad_norm': 8.614542961120605, 'learning_rate': 8.625e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2678, 'grad_norm': 9.498165130615234, 'learning_rate': 8.562500000000001e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2496, 'grad_norm': 8.916559219360352, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2451, 'grad_norm': 10.241991996765137, 'learning_rate': 8.437500000000002e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2061, 'grad_norm': 10.022878646850586, 'learning_rate': 8.375e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2926, 'grad_norm': 9.474188804626465, 'learning_rate': 8.3125e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2484, 'grad_norm': 8.465934753417969, 'learning_rate': 8.25e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3316, 'grad_norm': 9.828596115112305, 'learning_rate': 8.1875e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2605, 'grad_norm': 8.470405578613281, 'learning_rate': 8.125000000000001e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3365, 'grad_norm': 9.15129566192627, 'learning_rate': 8.062500000000001e-06, 'epoch': 0.23}\n",
      "{'loss': 3.4008, 'grad_norm': 8.965800285339355, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.23}\n",
      "{'loss': 3.3496, 'grad_norm': 13.670434951782227, 'learning_rate': 7.9375e-06, 'epoch': 0.23}\n",
      "{'loss': 3.2439, 'grad_norm': 8.236268997192383, 'learning_rate': 7.875e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3111, 'grad_norm': 9.195935249328613, 'learning_rate': 7.8125e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3127, 'grad_norm': 11.43917179107666, 'learning_rate': 7.75e-06, 'epoch': 0.24}\n",
      "{'loss': 3.324, 'grad_norm': 8.988606452941895, 'learning_rate': 7.6875e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2873, 'grad_norm': 9.373211860656738, 'learning_rate': 7.625e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2605, 'grad_norm': 9.274580001831055, 'learning_rate': 7.5625e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2355, 'grad_norm': 9.137128829956055, 'learning_rate': 7.5e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3725, 'grad_norm': 8.84530258178711, 'learning_rate': 7.4375e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3576, 'grad_norm': 11.46532917022705, 'learning_rate': 7.375e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3123, 'grad_norm': 8.47556209564209, 'learning_rate': 7.3125e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2799, 'grad_norm': 9.980643272399902, 'learning_rate': 7.25e-06, 'epoch': 0.24}\n",
      "{'loss': 3.4492, 'grad_norm': 9.21888542175293, 'learning_rate': 7.187499999999999e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3463, 'grad_norm': 8.902956008911133, 'learning_rate': 7.1249999999999995e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3088, 'grad_norm': 9.258707046508789, 'learning_rate': 7.0625e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2518, 'grad_norm': 9.520125389099121, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.1176, 'grad_norm': 9.279744148254395, 'learning_rate': 6.937500000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2705, 'grad_norm': 9.419087409973145, 'learning_rate': 6.875000000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.41, 'grad_norm': 9.160106658935547, 'learning_rate': 6.8125e-06, 'epoch': 0.24}\n",
      "{'loss': 3.1736, 'grad_norm': 9.265863418579102, 'learning_rate': 6.750000000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3531, 'grad_norm': 7.946325778961182, 'learning_rate': 6.687500000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2404, 'grad_norm': 10.179335594177246, 'learning_rate': 6.625000000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.1805, 'grad_norm': 8.7418794631958, 'learning_rate': 6.5625e-06, 'epoch': 0.24}\n",
      "{'loss': 3.34, 'grad_norm': 9.843316078186035, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.24}\n",
      "{'loss': 3.4328, 'grad_norm': 9.887351036071777, 'learning_rate': 6.437500000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.2148, 'grad_norm': 9.569820404052734, 'learning_rate': 6.375000000000001e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3025, 'grad_norm': 9.7747802734375, 'learning_rate': 6.3125e-06, 'epoch': 0.24}\n",
      "{'loss': 3.3281, 'grad_norm': 8.502416610717773, 'learning_rate': 6.25e-06, 'epoch': 0.24}\n",
      " 88%|████████████████████████████████▍    | 7000/8000 [1:11:49<10:08,  1.64it/s]Saving model checkpoint to ./output/checkpoint-7000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.5764, 'grad_norm': 9.152153968811035, 'learning_rate': 6.1875000000000005e-06, 'epoch': 0.24}\n",
      "{'loss': 3.4084, 'grad_norm': 8.674609184265137, 'learning_rate': 6.125e-06, 'epoch': 0.25}\n",
      "{'loss': 3.166, 'grad_norm': 9.9320650100708, 'learning_rate': 6.0625e-06, 'epoch': 0.25}\n",
      "{'loss': 3.0834, 'grad_norm': 8.189338684082031, 'learning_rate': 6e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3311, 'grad_norm': 8.494365692138672, 'learning_rate': 5.9375e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2055, 'grad_norm': 8.746585845947266, 'learning_rate': 5.875e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3701, 'grad_norm': 9.943405151367188, 'learning_rate': 5.812500000000001e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3258, 'grad_norm': 9.316030502319336, 'learning_rate': 5.750000000000001e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2791, 'grad_norm': 8.893383026123047, 'learning_rate': 5.6875e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2977, 'grad_norm': 8.786133766174316, 'learning_rate': 5.625e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3074, 'grad_norm': 8.651636123657227, 'learning_rate': 5.5625000000000005e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2949, 'grad_norm': 9.588081359863281, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3473, 'grad_norm': 8.937629699707031, 'learning_rate': 5.4375e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3672, 'grad_norm': 9.424104690551758, 'learning_rate': 5.375e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2641, 'grad_norm': 8.133459091186523, 'learning_rate': 5.3125e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3072, 'grad_norm': 9.42491626739502, 'learning_rate': 5.25e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3203, 'grad_norm': 8.666084289550781, 'learning_rate': 5.1875e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3602, 'grad_norm': 9.645066261291504, 'learning_rate': 5.125e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3766, 'grad_norm': 9.769830703735352, 'learning_rate': 5.0625e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2143, 'grad_norm': 8.476022720336914, 'learning_rate': 5e-06, 'epoch': 0.25}\n",
      "{'loss': 3.1734, 'grad_norm': 8.718024253845215, 'learning_rate': 4.937500000000001e-06, 'epoch': 0.25}\n",
      "{'loss': 3.232, 'grad_norm': 10.185525894165039, 'learning_rate': 4.875000000000001e-06, 'epoch': 0.25}\n",
      "{'loss': 3.0443, 'grad_norm': 9.398439407348633, 'learning_rate': 4.8125e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3576, 'grad_norm': 10.377367973327637, 'learning_rate': 4.75e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3758, 'grad_norm': 9.284710884094238, 'learning_rate': 4.6875000000000004e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3441, 'grad_norm': 12.782203674316406, 'learning_rate': 4.625e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2645, 'grad_norm': 9.123770713806152, 'learning_rate': 4.5625e-06, 'epoch': 0.25}\n",
      "{'loss': 3.358, 'grad_norm': 9.134462356567383, 'learning_rate': 4.5e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3459, 'grad_norm': 9.74669075012207, 'learning_rate': 4.4375e-06, 'epoch': 0.25}\n",
      "{'loss': 3.2572, 'grad_norm': 9.63363265991211, 'learning_rate': 4.375e-06, 'epoch': 0.25}\n",
      "{'loss': 3.3076, 'grad_norm': 10.469243049621582, 'learning_rate': 4.3125e-06, 'epoch': 0.26}\n",
      "{'loss': 3.3658, 'grad_norm': 9.40615177154541, 'learning_rate': 4.250000000000001e-06, 'epoch': 0.26}\n",
      "{'loss': 3.3805, 'grad_norm': 9.689689636230469, 'learning_rate': 4.1875e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2773, 'grad_norm': 9.052172660827637, 'learning_rate': 4.125e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2549, 'grad_norm': 9.11589527130127, 'learning_rate': 4.0625000000000005e-06, 'epoch': 0.26}\n",
      "{'loss': 3.1898, 'grad_norm': 9.201292991638184, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2635, 'grad_norm': 9.228212356567383, 'learning_rate': 3.9375e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2313, 'grad_norm': 8.810526847839355, 'learning_rate': 3.875e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2576, 'grad_norm': 10.367290496826172, 'learning_rate': 3.8125e-06, 'epoch': 0.26}\n",
      "{'loss': 3.232, 'grad_norm': 10.06978702545166, 'learning_rate': 3.75e-06, 'epoch': 0.26}\n",
      "{'loss': 3.417, 'grad_norm': 10.00426959991455, 'learning_rate': 3.6875e-06, 'epoch': 0.26}\n",
      "{'loss': 3.325, 'grad_norm': 9.62340259552002, 'learning_rate': 3.625e-06, 'epoch': 0.26}\n",
      "{'loss': 3.218, 'grad_norm': 8.80128288269043, 'learning_rate': 3.5624999999999998e-06, 'epoch': 0.26}\n",
      "{'loss': 3.4158, 'grad_norm': 9.318397521972656, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2406, 'grad_norm': 8.555147171020508, 'learning_rate': 3.4375000000000005e-06, 'epoch': 0.26}\n",
      "{'loss': 3.1971, 'grad_norm': 10.378300666809082, 'learning_rate': 3.3750000000000003e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2174, 'grad_norm': 9.25289535522461, 'learning_rate': 3.3125000000000005e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2936, 'grad_norm': 8.226605415344238, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2441, 'grad_norm': 8.735755920410156, 'learning_rate': 3.1875000000000004e-06, 'epoch': 0.26}\n",
      "{'loss': 3.0262, 'grad_norm': 9.515275955200195, 'learning_rate': 3.125e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2496, 'grad_norm': 8.658772468566895, 'learning_rate': 3.0625e-06, 'epoch': 0.26}\n",
      "{'loss': 3.3115, 'grad_norm': 8.707122802734375, 'learning_rate': 3e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2893, 'grad_norm': 8.065455436706543, 'learning_rate': 2.9375e-06, 'epoch': 0.26}\n",
      "{'loss': 3.3873, 'grad_norm': 9.62684154510498, 'learning_rate': 2.8750000000000004e-06, 'epoch': 0.26}\n",
      "{'loss': 3.3734, 'grad_norm': 9.635247230529785, 'learning_rate': 2.8125e-06, 'epoch': 0.26}\n",
      "{'loss': 3.3408, 'grad_norm': 9.491165161132812, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.26}\n",
      "{'loss': 3.2889, 'grad_norm': 10.099390029907227, 'learning_rate': 2.6875e-06, 'epoch': 0.26}\n",
      "{'loss': 3.3557, 'grad_norm': 9.275714874267578, 'learning_rate': 2.625e-06, 'epoch': 0.26}\n",
      "{'loss': 3.324, 'grad_norm': 8.344123840332031, 'learning_rate': 2.5625e-06, 'epoch': 0.26}\n",
      "{'loss': 3.4152, 'grad_norm': 9.498821258544922, 'learning_rate': 2.5e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3141, 'grad_norm': 9.877585411071777, 'learning_rate': 2.4375000000000004e-06, 'epoch': 0.27}\n",
      "{'loss': 3.1969, 'grad_norm': 9.873847961425781, 'learning_rate': 2.375e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3193, 'grad_norm': 10.562135696411133, 'learning_rate': 2.3125e-06, 'epoch': 0.27}\n",
      "{'loss': 3.2195, 'grad_norm': 9.568358421325684, 'learning_rate': 2.25e-06, 'epoch': 0.27}\n",
      "{'loss': 3.309, 'grad_norm': 10.118738174438477, 'learning_rate': 2.1875e-06, 'epoch': 0.27}\n",
      "{'loss': 3.223, 'grad_norm': 8.806241035461426, 'learning_rate': 2.1250000000000004e-06, 'epoch': 0.27}\n",
      "{'loss': 3.2938, 'grad_norm': 8.37044906616211, 'learning_rate': 2.0625e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3799, 'grad_norm': 9.366694450378418, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.27}\n",
      "{'loss': 3.252, 'grad_norm': 9.572522163391113, 'learning_rate': 1.9375e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3078, 'grad_norm': 9.881860733032227, 'learning_rate': 1.875e-06, 'epoch': 0.27}\n",
      "{'loss': 3.326, 'grad_norm': 10.213594436645508, 'learning_rate': 1.8125e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3354, 'grad_norm': 8.543850898742676, 'learning_rate': 1.7500000000000002e-06, 'epoch': 0.27}\n",
      "{'loss': 3.2996, 'grad_norm': 9.130033493041992, 'learning_rate': 1.6875000000000001e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3398, 'grad_norm': 9.741662979125977, 'learning_rate': 1.6250000000000001e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3834, 'grad_norm': 8.221464157104492, 'learning_rate': 1.5625e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3385, 'grad_norm': 10.148921966552734, 'learning_rate': 1.5e-06, 'epoch': 0.27}\n",
      "{'loss': 3.273, 'grad_norm': 9.273957252502441, 'learning_rate': 1.4375000000000002e-06, 'epoch': 0.27}\n",
      "{'loss': 3.2437, 'grad_norm': 9.127028465270996, 'learning_rate': 1.3750000000000002e-06, 'epoch': 0.27}\n",
      "{'loss': 3.1975, 'grad_norm': 10.42213249206543, 'learning_rate': 1.3125e-06, 'epoch': 0.27}\n",
      "{'loss': 3.2541, 'grad_norm': 8.344011306762695, 'learning_rate': 1.25e-06, 'epoch': 0.27}\n",
      "{'loss': 3.4164, 'grad_norm': 8.947111129760742, 'learning_rate': 1.1875e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3219, 'grad_norm': 8.64414119720459, 'learning_rate': 1.125e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3117, 'grad_norm': 8.66271686553955, 'learning_rate': 1.0625000000000002e-06, 'epoch': 0.27}\n",
      "{'loss': 3.3609, 'grad_norm': 8.827528953552246, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.27}\n",
      "{'loss': 3.4262, 'grad_norm': 9.494163513183594, 'learning_rate': 9.375e-07, 'epoch': 0.27}\n",
      "{'loss': 3.1943, 'grad_norm': 8.681544303894043, 'learning_rate': 8.750000000000001e-07, 'epoch': 0.27}\n",
      "{'loss': 3.2482, 'grad_norm': 9.778348922729492, 'learning_rate': 8.125000000000001e-07, 'epoch': 0.27}\n",
      "{'loss': 3.1922, 'grad_norm': 9.620586395263672, 'learning_rate': 7.5e-07, 'epoch': 0.28}\n",
      "{'loss': 3.167, 'grad_norm': 9.028504371643066, 'learning_rate': 6.875000000000001e-07, 'epoch': 0.28}\n",
      "{'loss': 3.315, 'grad_norm': 10.459146499633789, 'learning_rate': 6.25e-07, 'epoch': 0.28}\n",
      "{'loss': 3.4902, 'grad_norm': 10.865182876586914, 'learning_rate': 5.625e-07, 'epoch': 0.28}\n",
      "{'loss': 3.2785, 'grad_norm': 8.656452178955078, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.28}\n",
      "{'loss': 3.2314, 'grad_norm': 10.190356254577637, 'learning_rate': 4.3750000000000005e-07, 'epoch': 0.28}\n",
      "{'loss': 3.2768, 'grad_norm': 8.977591514587402, 'learning_rate': 3.75e-07, 'epoch': 0.28}\n",
      "{'loss': 3.2908, 'grad_norm': 9.672901153564453, 'learning_rate': 3.125e-07, 'epoch': 0.28}\n",
      "{'loss': 3.2283, 'grad_norm': 8.661149024963379, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.28}\n",
      "{'loss': 3.4359, 'grad_norm': 10.17805290222168, 'learning_rate': 1.875e-07, 'epoch': 0.28}\n",
      "{'loss': 3.2602, 'grad_norm': 9.33164119720459, 'learning_rate': 1.2500000000000002e-07, 'epoch': 0.28}\n",
      "{'loss': 3.2984, 'grad_norm': 8.709664344787598, 'learning_rate': 6.250000000000001e-08, 'epoch': 0.28}\n",
      "{'loss': 3.2004, 'grad_norm': 9.867477416992188, 'learning_rate': 0.0, 'epoch': 0.28}\n",
      "100%|█████████████████████████████████████| 8000/8000 [1:21:46<00:00,  1.87it/s]/environment/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.58s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:18<00:07,  7.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.951722, 'eval_rouge-2': 7.139128, 'eval_rouge-l': 25.479732000000002, 'eval_bleu-4': 0.034769183638261446, 'eval_runtime': 23.9223, 'eval_samples_per_second': 2.09, 'eval_steps_per_second': 0.167, 'epoch': 0.28}\n",
      "100%|█████████████████████████████████████| 8000/8000 [1:22:10<00:00,  1.87it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:20<00:00,  5.24s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-8000\n",
      "loading configuration file config.json from cache at /home/featurize/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/a5ba5501eb873d40d48bd0983bd2a8dd006bb838/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 4931.297, 'train_samples_per_second': 6.489, 'train_steps_per_second': 1.622, 'train_loss': 3.361751708984375, 'epoch': 0.28}\n",
      "100%|█████████████████████████████████████| 8000/8000 [1:22:11<00:00,  1.62it/s]\n",
      "/environment/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [10:45<00:00,  9.64s/it]\n"
     ]
    }
   ],
   "source": [
    "!python3 finetune_hf.py  data/AdvertiseGen_fix  THUDM/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {},
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-3000  checkpoint-5000  checkpoint-7000  runs\n",
      "checkpoint-2000  checkpoint-4000  checkpoint-6000  checkpoint-8000\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  2.12it/s]\n",
      "tokenizer_config.json: 100%|███████████████| 1.40k/1.40k [00:00<00:00, 4.22MB/s]\n",
      "tokenization_chatglm.py: 100%|█████████████| 13.0k/13.0k [00:00<00:00, 32.3MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "tokenizer.model: 100%|██████████████████████| 1.02M/1.02M [00:00<00:00, 144MB/s]\n",
      "special_tokens_map.json: 100%|███████████████| 3.00/3.00 [00:00<00:00, 8.84kB/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "2024-04-04 15:56:53.738634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-04 15:56:54.849235: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "这款连衣裙采用拼接设计，不规则的裙摆，显得十分有层次感，让整体看起来更有层次感。袖口设计上采用木耳边拼接，十分有层次感，不规则的裙摆，让裙身显得更加有层次感，十分性感。腰间压褶设计，显瘦效果十分好，拉链套头设计，穿脱方便。百褶的裙摆，十分优雅。\n"
     ]
    }
   ],
   "source": [
    "!python3 inference_hf.py output/checkpoint-8000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {},
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
